#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
MCP (Model Computing Platform) API Server

This module implements a FastAPI server that receives PDF files,
processes them (potentially with AI models), and returns results.
"""

import os
import json
import logging
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

import uvicorn
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware

# Optional: Import model loader if AI processing is needed
try:
    from .model_loader import ModelLoader
    HAS_MODEL_LOADER = True
except ImportError:
    HAS_MODEL_LOADER = False

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("mcp.server")

# Create FastAPI app
app = FastAPI(
    title="MCP API Server",
    description="API for processing PDF papers with AI models",
    version="0.1.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global model instance (if using AI models)
model = None


@app.on_event("startup")
async def startup_event():
    """
    Initialize resources on startup
    """
    global model
    
    logger.info("Starting MCP API Server")
    
    # Initialize model if available
    if HAS_MODEL_LOADER:
        try:
            logger.info("Loading AI model...")
            model = ModelLoader()
            logger.info("AI model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load AI model: {str(e)}")
            # Continue without model - will use mock processing


@app.on_event("shutdown")
async def shutdown_event():
    """
    Clean up resources on shutdown
    """
    global model
    
    logger.info("Shutting down MCP API Server")
    
    # Clean up model resources if needed
    if model is not None:
        try:
            logger.info("Unloading AI model...")
            del model
            model = None
            logger.info("AI model unloaded successfully")
        except Exception as e:
            logger.error(f"Error unloading AI model: {str(e)}")


@app.get("/")
async def root():
    """
    Root endpoint - health check
    """
    return {"status": "ok", "message": "MCP API Server is running"}


@app.get("/health")
async def health_check():
    """
    Health check endpoint
    """
    return {
        "status": "ok",
        "timestamp": datetime.now().isoformat(),
        "model_loaded": model is not None if HAS_MODEL_LOADER else "not_applicable"
    }


def process_pdf_with_model(pdf_path: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process a PDF file with the AI model
    
    Args:
        pdf_path (str): Path to the PDF file
        metadata (Dict[str, Any]): Metadata about the PDF
        
    Returns:
        Dict[str, Any]: Processing results
    """
    global model
    
    if model is not None and HAS_MODEL_LOADER:
        try:
            # Use the model to process the PDF
            return model.process_pdf(pdf_path, metadata)
        except Exception as e:
            logger.error(f"Error processing PDF with model: {str(e)}")
            # Fall back to mock processing
    
    # Mock processing when no model is available
    logger.info("Using mock processing (no AI model available)")
    
    title = metadata.get("title", "Unknown Title")
    authors = metadata.get("authors", "Unknown Authors")
    
    return {
        "title": title,
        "authors": authors,
        "processed_at": datetime.now().isoformat(),
        "summary": f"This is a mock summary for '{title}' by {authors}. In the future, this will be generated by an AI model.",
        "keywords": ["mock", "placeholder", "ai", "pdf", "processing"],
        "status": "success",
        "model_used": "none (mock processing)"
    }


async def _process_pdf_file(pdf: UploadFile, metadata: str):
    """
    Internal function to process an uploaded PDF file
    
    Args:
        pdf (UploadFile): The PDF file to process
        metadata (str): JSON string with metadata about the PDF
        
    Returns:
        JSONResponse: Processing results
    """
    logger.info(f"Processing uploaded PDF: {pdf.filename}")
    
    # Parse metadata
    try:
        metadata_dict = json.loads(metadata)
    except json.JSONDecodeError:
        logger.warning("Invalid metadata format, using empty dict")
        metadata_dict = {}
    
    # Add filename to metadata if not present
    if "title" not in metadata_dict and pdf.filename:
        metadata_dict["title"] = Path(pdf.filename).stem
    
    # Save the uploaded file temporarily
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
            temp_path = temp_file.name
            content = await pdf.read()
            temp_file.write(content)
        
        logger.info(f"PDF saved temporarily at {temp_path}")
        
        # Process the PDF
        result = process_pdf_with_model(temp_path, metadata_dict)
        
        # Clean up the temporary file
        os.unlink(temp_path)
        logger.info(f"Temporary PDF file removed: {temp_path}")
        
        return JSONResponse(content=result)
        
    except Exception as e:
        logger.error(f"Error processing PDF: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing PDF: {str(e)}")


@app.post("/process")
async def process_pdf(
    pdf: UploadFile = File(...),
    metadata: str = Form("{}") 
):
    """
    Process a PDF file and return results
    
    Args:
        pdf (UploadFile): The PDF file to process
        metadata (str): JSON string with metadata about the PDF
        
    Returns:
        JSONResponse: Processing results
    """
    return await _process_pdf_file(pdf, metadata)


@app.post("/process_pdf")
async def process_pdf_path(file_path: Dict[str, str]):
    """Process a PDF file from a local file path
    
    Args:
        file_path (Dict[str, str]): Dictionary containing the file_path key
        
    Returns:
        JSONResponse: Processing results
    """
    logger.info(f"Received PDF path: {file_path['file_path']}")
    
    # Check if file exists
    pdf_path = file_path['file_path']
    if not os.path.exists(pdf_path):
        raise HTTPException(status_code=404, detail=f"File not found: {pdf_path}")
    
    # Get metadata if available
    metadata_dict = {}
    metadata_path = f"{os.path.splitext(pdf_path)[0]}.json"
    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata_dict = json.load(f)
        except json.JSONDecodeError:
            logger.warning(f"Invalid metadata format for {pdf_path}")
    
    # Add filename to metadata if not present
    if "title" not in metadata_dict:
        metadata_dict["title"] = os.path.basename(os.path.splitext(pdf_path)[0])
    
    # Process the PDF
    result = process_pdf_with_model(pdf_path, metadata_dict)
    
    return JSONResponse(content=result)


def start_server(host: str = "0.0.0.0", port: int = 8000, reload: bool = False):
    """
    Start the FastAPI server
    
    Args:
        host (str): Host to bind the server to
        port (int): Port to bind the server to
        reload (bool): Whether to enable auto-reload
    """
    uvicorn.run("mcp.server:app", host=host, port=port, reload=reload)


if __name__ == "__main__":
    # This allows the module to be run directly for testing
    start_server(reload=True)
